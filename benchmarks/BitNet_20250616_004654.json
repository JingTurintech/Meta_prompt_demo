{
  "metadata": {
    "collected_at": "20250616_004654",
    "project_info": {
      "project_id": "114ba2fa-8bae-4e19-8f46-3fbef23b4a98",
      "name": "BitNet",
      "description": "https://github.com/microsoft/BitNet",
      "language": "c"
    }
  },
  "code_snippets": [
    {
      "id": "311024d3-31c9-4e28-9360-3e1b10566eb1",
      "content": "static void ggml_barrier(struct ggml_threadpool * tp) {     int n_threads = atomic_load_explicit(&tp->n_threads_cur, memory_order_relaxed);     if (n_threads == 1) {         return;     }  #ifdef GGML_USE_OPENMP     #pragma omp barrier #else     int n_passed = atomic_load_explicit(&tp->n_barrier_passed, memory_order_relaxed);      // enter barrier (full seq-cst fence)     int n_barrier = atomic_fetch_add_explicit(&tp->n_barrier, 1, memory_order_seq_cst);      if (n_barrier == (n_threads - 1)) {         // last thread         atomic_store_explicit(&tp->n_barrier, 0, memory_order_relaxed);          // exit barrier (fill seq-cst fence)         atomic_fetch_add_explicit(&tp->n_barrier_passed, 1, memory_order_seq_cst);         return;     }      // wait for other threads     while (atomic_load_explicit(&tp->n_barrier_passed, memory_order_relaxed) == n_passed) {         ggml_thread_cpu_relax();     }      // exit barrier (full seq-cst fence)     // TSAN doesn't support standalone fence yet, we use a dummy read-modify-write instead     #ifdef GGML_TSAN_ENABLED     atomic_fetch_add_explicit(&tp->n_barrier_passed, 0, memory_order_seq_cst);     #else     atomic_thread_fence(memory_order_seq_cst);     #endif #endif }",
      "construct_id": "97ab3b7b-25c1-49eb-aebb-51764115ae2d",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.231678",
        "updated_at": "2025-06-12T17:27:13.231703",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 3289,
          "end": 3325
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1",
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER"
        ]
      }
    },
    {
      "id": "92ad11dc-083a-4ac3-8270-f0da99a7ca91",
      "content": "static void ggml_vec_dot_f16(int n, float * restrict s, size_t bs, ggml_fp16_t * restrict x, size_t bx, ggml_fp16_t * restrict y, size_t by, int nrc) {     assert(nrc == 1);     UNUSED(nrc);     UNUSED(bx);     UNUSED(by);     UNUSED(bs);      ggml_float sumf = 0.0;  #if defined(GGML_SIMD)     const int np = (n & ~(GGML_F16_STEP - 1));      GGML_F16_VEC sum[GGML_F16_ARR] = { GGML_F16_VEC_ZERO };      GGML_F16_VEC ax[GGML_F16_ARR];     GGML_F16_VEC ay[GGML_F16_ARR];      for (int i = 0; i < np; i += GGML_F16_STEP) {         for (int j = 0; j < GGML_F16_ARR; j++) {             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);              sum[j] = GGML_F16_VEC_FMA(sum[j], ax[j], ay[j]);         }     }      // reduce sum0..sum3 to sum0     GGML_F16_VEC_REDUCE(sumf, sum);      // leftovers     for (int i = np; i < n; ++i) {         sumf += (ggml_float)(GGML_FP16_TO_FP32(x[i])*GGML_FP16_TO_FP32(y[i]));     } #else     for (int i = 0; i < n; ++i) {         sumf += (ggml_float)(GGML_FP16_TO_FP32(x[i])*GGML_FP16_TO_FP32(y[i]));     } #endif      *s = sumf; }",
      "construct_id": "a708e396-2e2a-431c-96cc-ec714254ced1",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.232702",
        "updated_at": "2025-06-12T17:27:13.232716",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 2286,
          "end": 2326
        },
        "tokens": null,
        "construct_tags": [
          "RANK 2",
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER"
        ]
      }
    },
    {
      "id": "332f99e5-ead2-485f-89c9-11a0ecc46b23",
      "content": "void ggml_vec_dot_i2_i8_s(int n, float * s, size_t bs, const void * vx, size_t bx, const void * vy, size_t by, int nrc) {     const uint8_t *    x = (uint8_t *)vx;     const int8_t  *    y = (int8_t *)vy;      const int nb = n / QK_I2_S;     const int group32_num = nb / 32;     const int la_num = nb % 32;     const int groupla_num = nb % 32 != 0 ? 1 : 0;  #if defined(__AVX2__)      __m256i mask = _mm256_set1_epi8(0x03);     __m256i accu = _mm256_setzero_si256();      for (int i=0; i < group32_num; i++){         __m256i accu32 = _mm256_setzero_si256();         for (int j=0; j < 32; j++) {         // 128 index         __m256i xq8_3 = _mm256_loadu_si256((const __m256i*)(x + i * 32 * 32 + j * 32));         __m256i xq8_2 = _mm256_srli_epi16(xq8_3, 2);         __m256i xq8_1 = _mm256_srli_epi16(xq8_3, 4);         __m256i xq8_0 = _mm256_srli_epi16(xq8_3, 6);          // each 32 index         xq8_3 = _mm256_and_si256(xq8_3, mask);         xq8_2 = _mm256_and_si256(xq8_2, mask);         xq8_1 = _mm256_and_si256(xq8_1, mask);         xq8_0 = _mm256_and_si256(xq8_0, mask);          // each 32 index         __m256i yq8_0 = _mm256_loadu_si256((const __m256i*)(y + i * 128 * 32 + j * 128 + 0));         __m256i yq8_1 = _mm256_loadu_si256((const __m256i*)(y + i * 128 * 32 + j * 128 + 32));         __m256i yq8_2 = _mm256_loadu_si256((const __m256i*)(y + i * 128 * 32 + j * 128 + 64));         __m256i yq8_3 = _mm256_loadu_si256((const __m256i*)(y + i * 128 * 32 + j * 128 + 96));          // 128 index accumulation add         // split into 32 accumulation block         // each block each 128 index accumulated 4index         // each index maximum 256         // each block maximum 4 * 256         // each block accumulation maximum 127 * 256         // each 32 group index (128 index in one group) needs cast to int32         xq8_0 = _mm256_maddubs_epi16(xq8_0, yq8_0);         xq8_1 = _mm256_maddubs_epi16(xq8_1, yq8_1);         xq8_2 = _mm256_maddubs_epi16(xq8_2, yq8_2);         xq8_3 = _mm256_maddubs_epi16(xq8_3, yq8_3);          accu32 = _mm256_add_epi16(accu32, _mm256_add_epi16(xq8_0, xq8_1));         accu32 = _mm256_add_epi16(accu32, _mm256_add_epi16(xq8_2, xq8_3));         }         accu = _mm256_add_epi32(_mm256_madd_epi16(accu32, _mm256_set1_epi16(1)), accu);     }      for (int i = 0; i < groupla_num; i++){         __m256i accula = _mm256_setzero_si256();         for (int j = 0; j < la_num; j++) {         // 128 index         __m256i xq8_3 = _mm256_loadu_si256((const __m256i*)(x + group32_num * 32 * 32 + j * 32));         __m256i xq8_2 = _mm256_srli_epi16(xq8_3, 2);         __m256i xq8_1 = _mm256_srli_epi16(xq8_3, 4);         __m256i xq8_0 = _mm256_srli_epi16(xq8_3, 6);          // each 32 index         xq8_3 = _mm256_and_si256(xq8_3, mask);         xq8_2 = _mm256_and_si256(xq8_2, mask);         xq8_1 = _mm256_and_si256(xq8_1, mask);         xq8_0 = _mm256_and_si256(xq8_0, mask);          // each 32 index         __m256i yq8_0 = _mm256_loadu_si256((const __m256i*)(y + group32_num * 128 * 32 + j * 128 + 0));         __m256i yq8_1 = _mm256_loadu_si256((const __m256i*)(y + group32_num * 128 * 32 + j * 128 + 32));         __m256i yq8_2 = _mm256_loadu_si256((const __m256i*)(y + group32_num * 128 * 32 + j * 128 + 64));         __m256i yq8_3 = _mm256_loadu_si256((const __m256i*)(y + group32_num * 128 * 32 + j * 128 + 96));          // 128 index accumulation add         // split into 32 accumulation block         // each block each 128 index accumulated 4index         // each index maximum 256         // each block maximum 4 * 256         // each block accumulation maximum 127 * 256         // each 32 group index (128 index in one group) needs cast to int32         xq8_0 = _mm256_maddubs_epi16(xq8_0, yq8_0);         xq8_1 = _mm256_maddubs_epi16(xq8_1, yq8_1);         xq8_2 = _mm256_maddubs_epi16(xq8_2, yq8_2);         xq8_3 = _mm256_maddubs_epi16(xq8_3, yq8_3);          accula = _mm256_add_epi16(accula, _mm256_add_epi16(xq8_0, xq8_1));         accula = _mm256_add_epi16(accula, _mm256_add_epi16(xq8_2, xq8_3));         }         accu = _mm256_add_epi32(accu, _mm256_madd_epi16(accula, _mm256_set1_epi16(1)));     }     int sumi = hsum_i32_8(accu);     *s = (float)sumi;  #elif defined(__ARM_NEON)      int32x4_t accu_0 = vdupq_n_s32(0);     int32x4_t accu_1 = vdupq_n_s32(0);     int32x4_t accu_2 = vdupq_n_s32(0);     int32x4_t accu_3 = vdupq_n_s32(0);     const uint8x16_t mask = vdupq_n_u8(3);      for (int i=0; i < group32_num; i++) {  #if defined(__ARM_FEATURE_DOTPROD)  #else         int16x8_t accu32_0 = vdupq_n_s16(0);         int16x8_t accu32_1 = vdupq_n_s16(0);         int16x8_t accu32_2 = vdupq_n_s16(0);         int16x8_t accu32_3 = vdupq_n_s16(0); #endif          for (int j=0; j < 32; j++) {             uint8x16_t xq8_6 = vld1q_u8(x + i * 32 * 32 + j * 32);             uint8x16_t xq8_7 = vld1q_u8(x + i * 32 * 32 + j * 32 + 16);             uint8x16_t xq8_4 = vshrq_n_u8(xq8_6, 2);             uint8x16_t xq8_5 = vshrq_n_u8(xq8_7, 2);             uint8x16_t xq8_2 = vshrq_n_u8(xq8_6, 4);             uint8x16_t xq8_3 = vshrq_n_u8(xq8_7, 4);             uint8x16_t xq8_0 = vshrq_n_u8(xq8_6, 6);             uint8x16_t xq8_1 = vshrq_n_u8(xq8_7, 6);              int8x16_t q8_0 = vreinterpretq_s8_u8(vandq_u8(xq8_0, mask));             int8x16_t q8_1 = vreinterpretq_s8_u8(vandq_u8(xq8_1, mask));             int8x16_t q8_2 = vreinterpretq_s8_u8(vandq_u8(xq8_2, mask));             int8x16_t q8_3 = vreinterpretq_s8_u8(vandq_u8(xq8_3, mask));             int8x16_t q8_4 = vreinterpretq_s8_u8(vandq_u8(xq8_4, mask));             int8x16_t q8_5 = vreinterpretq_s8_u8(vandq_u8(xq8_5, mask));             int8x16_t q8_6 = vreinterpretq_s8_u8(vandq_u8(xq8_6, mask));             int8x16_t q8_7 = vreinterpretq_s8_u8(vandq_u8(xq8_7, mask));              const int8x16_t yq8_0 = vld1q_s8(y + i * 128 * 32 + j * 128 + 0);             const int8x16_t yq8_1 = vld1q_s8(y + i * 128 * 32 + j * 128 + 16);             const int8x16_t yq8_2 = vld1q_s8(y + i * 128 * 32 + j * 128 + 32);             const int8x16_t yq8_3 = vld1q_s8(y + i * 128 * 32 + j * 128 + 48);             const int8x16_t yq8_4 = vld1q_s8(y + i * 128 * 32 + j * 128 + 64);             const int8x16_t yq8_5 = vld1q_s8(y + i * 128 * 32 + j * 128 + 80);             const int8x16_t yq8_6 = vld1q_s8(y + i * 128 * 32 + j * 128 + 96);             const int8x16_t yq8_7 = vld1q_s8(y + i * 128 * 32 + j * 128 + 112);  #if defined(__ARM_FEATURE_DOTPROD)             accu_0 = vdotq_s32(accu_0, q8_0, yq8_0);             accu_1 = vdotq_s32(accu_1, q8_1, yq8_1);             accu_2 = vdotq_s32(accu_2, q8_2, yq8_2);             accu_3 = vdotq_s32(accu_3, q8_3, yq8_3);             accu_0 = vdotq_s32(accu_0, q8_4, yq8_4);             accu_1 = vdotq_s32(accu_1, q8_5, yq8_5);             accu_2 = vdotq_s32(accu_2, q8_6, yq8_6);             accu_3 = vdotq_s32(accu_3, q8_7, yq8_7); #else             accu32_0 = vmlal_s8(accu32_0, vget_low_s8(q8_0), vget_low_s8(yq8_0));             accu32_1 = vmlal_s8(accu32_1, vget_high_s8(q8_0), vget_high_s8(yq8_0));             accu32_2 = vmlal_s8(accu32_2, vget_low_s8(q8_1), vget_low_s8(yq8_1));             accu32_3 = vmlal_s8(accu32_3, vget_high_s8(q8_1), vget_high_s8(yq8_1));             accu32_0 = vmlal_s8(accu32_0, vget_low_s8(q8_2), vget_low_s8(yq8_2));             accu32_1 = vmlal_s8(accu32_1, vget_high_s8(q8_2), vget_high_s8(yq8_2));             accu32_2 = vmlal_s8(accu32_2, vget_low_s8(q8_3), vget_low_s8(yq8_3));             accu32_3 = vmlal_s8(accu32_3, vget_high_s8(q8_3), vget_high_s8(yq8_3));             accu32_0 = vmlal_s8(accu32_0, vget_low_s8(q8_4), vget_low_s8(yq8_4));             accu32_1 = vmlal_s8(accu32_1, vget_high_s8(q8_4), vget_high_s8(yq8_4));             accu32_2 = vmlal_s8(accu32_2, vget_low_s8(q8_5), vget_low_s8(yq8_5));             accu32_3 = vmlal_s8(accu32_3, vget_high_s8(q8_5), vget_high_s8(yq8_5));             accu32_0 = vmlal_s8(accu32_0, vget_low_s8(q8_6), vget_low_s8(yq8_6));             accu32_1 = vmlal_s8(accu32_1, vget_high_s8(q8_6), vget_high_s8(yq8_6));             accu32_2 = vmlal_s8(accu32_2, vget_low_s8(q8_7), vget_low_s8(yq8_7));             accu32_3 = vmlal_s8(accu32_3, vget_high_s8(q8_7), vget_high_s8(yq8_7)); #endif         }  #if defined(__ARM_FEATURE_DOTPROD)  #else         accu_0 = vaddq_s32(accu_0, vmovl_s16(vget_low_s16(accu32_0)));         accu_0 = vaddq_s32(accu_0, vmovl_high_s16(accu32_0));         accu_1 = vaddq_s32(accu_1, vmovl_s16(vget_low_s16(accu32_1)));         accu_1 = vaddq_s32(accu_1, vmovl_high_s16(accu32_1));         accu_2 = vaddq_s32(accu_2, vmovl_s16(vget_low_s16(accu32_2)));         accu_2 = vaddq_s32(accu_2, vmovl_high_s16(accu32_2));         accu_3 = vaddq_s32(accu_3, vmovl_s16(vget_low_s16(accu32_3)));         accu_3 = vaddq_s32(accu_3, vmovl_high_s16(accu32_3)); #endif     }      for (int i = 0; i < groupla_num; i++){ #if defined(__ARM_FEATURE_DOTPROD)  #else         int16x8_t accula_0 = vdupq_n_s16(0);         int16x8_t accula_1 = vdupq_n_s16(0);         int16x8_t accula_2 = vdupq_n_s16(0);         int16x8_t accula_3 = vdupq_n_s16(0); #endif         for (int j = 0; j < la_num; j++) {             uint8x16_t xq8_6 = vld1q_u8(x + group32_num * 32 * 32 + j * 32);             uint8x16_t xq8_7 = vld1q_u8(x + group32_num * 32 * 32 + j * 32 + 16);             uint8x16_t xq8_4 = vshrq_n_u8(xq8_6, 2);             uint8x16_t xq8_5 = vshrq_n_u8(xq8_7, 2);             uint8x16_t xq8_2 = vshrq_n_u8(xq8_6, 4);             uint8x16_t xq8_3 = vshrq_n_u8(xq8_7, 4);             uint8x16_t xq8_0 = vshrq_n_u8(xq8_6, 6);             uint8x16_t xq8_1 = vshrq_n_u8(xq8_7, 6);              int8x16_t q8_0 = vreinterpretq_s8_u8(vandq_u8(xq8_0, mask));             int8x16_t q8_1 = vreinterpretq_s8_u8(vandq_u8(xq8_1, mask));             int8x16_t q8_2 = vreinterpretq_s8_u8(vandq_u8(xq8_2, mask));             int8x16_t q8_3 = vreinterpretq_s8_u8(vandq_u8(xq8_3, mask));             int8x16_t q8_4 = vreinterpretq_s8_u8(vandq_u8(xq8_4, mask));             int8x16_t q8_5 = vreinterpretq_s8_u8(vandq_u8(xq8_5, mask));             int8x16_t q8_6 = vreinterpretq_s8_u8(vandq_u8(xq8_6, mask));             int8x16_t q8_7 = vreinterpretq_s8_u8(vandq_u8(xq8_7, mask));              const int8x16_t yq8_0 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 0);             const int8x16_t yq8_1 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 16);             const int8x16_t yq8_2 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 32);             const int8x16_t yq8_3 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 48);             const int8x16_t yq8_4 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 64);             const int8x16_t yq8_5 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 80);             const int8x16_t yq8_6 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 96);             const int8x16_t yq8_7 = vld1q_s8(y + group32_num * 128 * 32 + j * 128 + 112);  #if defined(__ARM_FEATURE_DOTPROD)             accu_0 = vdotq_s32(accu_0, q8_0, yq8_0);             accu_1 = vdotq_s32(accu_1, q8_1, yq8_1);             accu_2 = vdotq_s32(accu_2, q8_2, yq8_2);             accu_3 = vdotq_s32(accu_3, q8_3, yq8_3);             accu_0 = vdotq_s32(accu_0, q8_4, yq8_4);             accu_1 = vdotq_s32(accu_1, q8_5, yq8_5);             accu_2 = vdotq_s32(accu_2, q8_6, yq8_6);             accu_3 = vdotq_s32(accu_3, q8_7, yq8_7); #else             accula_0 = vmlal_s8(accula_0, vget_low_s8(q8_0), vget_low_s8(yq8_0));             accula_1 = vmlal_s8(accula_1, vget_high_s8(q8_0), vget_high_s8(yq8_0));             accula_2 = vmlal_s8(accula_2, vget_low_s8(q8_1), vget_low_s8(yq8_1));             accula_3 = vmlal_s8(accula_3, vget_high_s8(q8_1), vget_high_s8(yq8_1));             accula_0 = vmlal_s8(accula_0, vget_low_s8(q8_2), vget_low_s8(yq8_2));             accula_1 = vmlal_s8(accula_1, vget_high_s8(q8_2), vget_high_s8(yq8_2));             accula_2 = vmlal_s8(accula_2, vget_low_s8(q8_3), vget_low_s8(yq8_3));             accula_3 = vmlal_s8(accula_3, vget_high_s8(q8_3), vget_high_s8(yq8_3));             accula_0 = vmlal_s8(accula_0, vget_low_s8(q8_4), vget_low_s8(yq8_4));             accula_1 = vmlal_s8(accula_1, vget_high_s8(q8_4), vget_high_s8(yq8_4));             accula_2 = vmlal_s8(accula_2, vget_low_s8(q8_5), vget_low_s8(yq8_5));             accula_3 = vmlal_s8(accula_3, vget_high_s8(q8_5), vget_high_s8(yq8_5));             accula_0 = vmlal_s8(accula_0, vget_low_s8(q8_6), vget_low_s8(yq8_6));             accula_1 = vmlal_s8(accula_1, vget_high_s8(q8_6), vget_high_s8(yq8_6));             accula_2 = vmlal_s8(accula_2, vget_low_s8(q8_7), vget_low_s8(yq8_7));             accula_3 = vmlal_s8(accula_3, vget_high_s8(q8_7), vget_high_s8(yq8_7)); #endif         } #if defined(__ARM_FEATURE_DOTPROD)  #else         accu_0 = vaddq_s32(accu_0, vmovl_s16(vget_low_s16(accula_0)));         accu_0 = vaddq_s32(accu_0, vmovl_high_s16(accula_0));         accu_1 = vaddq_s32(accu_1, vmovl_s16(vget_low_s16(accula_1)));         accu_1 = vaddq_s32(accu_1, vmovl_high_s16(accula_1));         accu_2 = vaddq_s32(accu_2, vmovl_s16(vget_low_s16(accula_2)));         accu_2 = vaddq_s32(accu_2, vmovl_high_s16(accula_2));         accu_3 = vaddq_s32(accu_3, vmovl_s16(vget_low_s16(accula_3)));         accu_3 = vaddq_s32(accu_3, vmovl_high_s16(accula_3)); #endif     }     accu_0 = vaddq_s32(accu_0, accu_1);     accu_2 = vaddq_s32(accu_2, accu_3);     accu_0 = vaddq_s32(accu_0, accu_2);     int sumi = vaddlvq_s32(accu_0);     *s = (float)sumi;  #endif }",
      "construct_id": "0f482d67-7b12-41ce-b0ff-10c8e6399e51",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.234042",
        "updated_at": "2025-06-12T17:27:13.234055",
        "language": "cpp",
        "file": "src/ggml-bitnet-mad.cpp",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 94,
          "end": 363
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER",
          "RANK 3"
        ]
      }
    },
    {
      "id": "a8519ffe-3c71-4375-b904-fa5caa26c450",
      "content": "// ggml_compute_forward_mul_mat  static void ggml_compute_forward_mul_mat_one_chunk(     const struct ggml_compute_params * params,     struct ggml_tensor * dst,     const int64_t num_rows_per_vec_dot,     const int64_t ir0_start,     const int64_t ir0_end,     const int64_t ir1_start,     const int64_t ir1_end) {      const struct ggml_tensor * src0 = dst->src[0];     const struct ggml_tensor * src1 = dst->src[1];      GGML_TENSOR_BINARY_OP_LOCALS      const enum ggml_type type = src0->type;      const bool src1_cont = ggml_is_contiguous(src1);      ggml_vec_dot_t const vec_dot      = type_traits[type].vec_dot;     enum ggml_type const vec_dot_type = type_traits[type].vec_dot_type;      // broadcast factors     const int64_t r2 = ne12 / ne02;     const int64_t r3 = ne13 / ne03;      //printf(\"ir0_start = %6lld, ir0_end = %6lld, ir1_start = %6lld, ir1_end = %6lld\\n\", ir0_start, ir0_end, ir1_start, ir1_end);      // threads with no work simply yield (not sure if it helps)     if (ir0_start >= ir0_end || ir1_start >= ir1_end) {         return;     }      const void * wdata = (src1->type == vec_dot_type) ? src1->data : params->wdata;     const size_t row_size = ggml_row_size(vec_dot_type, ne10);      assert(ne12 % ne02 == 0);     assert(ne13 % ne03 == 0);      // block-tiling attempt     const int64_t blck_0 = 16;     const int64_t blck_1 = 16;      const size_t src1_col_stride = src1_cont || src1->type != vec_dot_type ? row_size : nb11;      // attempt to reduce false-sharing (does not seem to make a difference)     // 16 * 2, accounting for mmla kernels     float tmp[32];      const float * scale      = (float * )((uint8_t*) (src0->data) + (ne00 * ne01 / 4));     const float * act_scales = (const float*) ((const char *) wdata + (ne11 * ne10));     const int32_t * act_sums   = (const int32_t*) ((const char *) act_scales + (ne11) * sizeof(float));      for (int64_t iir1 = ir1_start; iir1 < ir1_end; iir1 += blck_1) {         for (int64_t iir0 = ir0_start; iir0 < ir0_end; iir0 += blck_0) {             for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir1_end; ir1 += num_rows_per_vec_dot) {                 const int64_t i13 = (ir1 / (ne12 * ne1));                 const int64_t i12 = (ir1 - i13 * ne12 * ne1) / ne1;                 const int64_t i11 = (ir1 - i13 * ne12 * ne1 - i12 * ne1);                  // broadcast src0 into src1                 const int64_t i03 = i13 / r3;                 const int64_t i02 = i12 / r2;                  const int64_t i1 = i11;                 const int64_t i2 = i12;                 const int64_t i3 = i13;                  const char * src0_row = (const char*)src0->data + (0 + i02 * nb02 + i03 * nb03);                  // desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides                 //       if it is, then we have either copied the data to params->wdata and made it contiguous or we are using                 //       the original src1 data pointer, so we should index using the indices directly                 // TODO: this is a bit of a hack, we should probably have a better way to handle this                 const char * src1_col = (const char*)wdata +                     (src1_cont || src1->type != vec_dot_type                         ? (i11 + i12 * ne11 + i13 * ne12 * ne11) * row_size                         : (i11 * nb11 + i12 * nb12 + i13 * nb13));                 // hack                 const char * src1_col_de = (const char*)wdata + (i11 * nb11 / 4);                  float * dst_col = (float*)((char*)dst->data + (i1 * nb1 + i2 * nb2 + i3 * nb3));                  //for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir0_end; ++ir0) {                 //    vec_dot(ne00, &dst_col[ir0], src0_row + ir0*nb01, src1_col);                 //}                  for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir0_end; ir0 += num_rows_per_vec_dot) {                     if (src0->type == GGML_TYPE_I2_S) {                         vec_dot(ne00, &tmp[ir0 - iir0], (num_rows_per_vec_dot > 1 ? 16 : 0), src0_row + ir0 * nb01 / 4, (num_rows_per_vec_dot > 1 ? nb01 : 0), src1_col_de, (num_rows_per_vec_dot > 1 ? src1_col_stride : 0), num_rows_per_vec_dot);                         tmp[ir0 - iir0] = (tmp[ir0 - iir0]  - act_sums[i1]) / (act_scales[i1]) * (*scale);                     } else {                         vec_dot(ne00, &tmp[ir0 - iir0], (num_rows_per_vec_dot > 1 ? 16 : 0), src0_row + ir0 * nb01, (num_rows_per_vec_dot > 1 ? nb01 : 0), src1_col, (num_rows_per_vec_dot > 1 ? src1_col_stride : 0), num_rows_per_vec_dot);                     }                 }                  for (int cn = 0; cn < num_rows_per_vec_dot; ++cn) {                     memcpy(&dst_col[iir0 + cn * nb1 / nb0], tmp + (cn * 16), (MIN(iir0 + blck_0, ir0_end) - iir0) * sizeof(float));                 }             }         }     } }",
      "construct_id": "ed0e05d2-3f39-4f23-b645-56d627a7f46f",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.235273",
        "updated_at": "2025-06-12T17:27:13.235287",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 12402,
          "end": 12505
        },
        "tokens": null,
        "construct_tags": [
          "RANK 4",
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER"
        ]
      }
    },
    {
      "id": "48f0027b-73ef-4ac1-8745-94c9d4146bc4",
      "content": "static void ggml_compute_forward_mul_mat(         const struct ggml_compute_params * params,               struct ggml_tensor * dst) {      const struct ggml_tensor * src0 = dst->src[0];     const struct ggml_tensor * src1 = dst->src[1];      GGML_TENSOR_BINARY_OP_LOCALS      const int ith = params->ith;     const int nth = params->nth;      const enum ggml_type type = src0->type;      enum ggml_type           const vec_dot_type         = type_traits[type].vec_dot_type;     ggml_from_float_t        const from_float           = type_traits[vec_dot_type].from_float;     ggml_from_float_to_mat_t const from_float_to_mat    = type_traits[vec_dot_type].from_float_to_mat;     int64_t                  const vec_dot_num_rows     = type_traits[type].nrows;     int64_t                  const matmul_num_cols      = type_traits[type].ncols;     int64_t                  const blck_size_interleave = type_traits[type].blck_size_interleave;     ggml_gemv_t              const gemv                 = type_traits[type].gemv;     ggml_gemm_t              const gemm                 = type_traits[type].gemm;      GGML_ASSERT(ne0 == ne01);     GGML_ASSERT(ne1 == ne11);     GGML_ASSERT(ne2 == ne12);     GGML_ASSERT(ne3 == ne13);      // we don't support permuted src0 or src1     GGML_ASSERT(nb00 == ggml_type_size(type));     GGML_ASSERT(nb10 == ggml_type_size(src1->type));      // dst cannot be transposed or permuted     GGML_ASSERT(nb0 == sizeof(float));     GGML_ASSERT(nb0 <= nb1);     GGML_ASSERT(nb1 <= nb2);     GGML_ASSERT(nb2 <= nb3);  // #ifndef GGML_BITNET_X86_TL2 //     if (src1->ne[1] <= 1 && src0->type != GGML_TYPE_TL1 && src0->type != GGML_TYPE_I2_S && src0->type != GGML_TYPE_TQ1_0 && src0->type != GGML_TYPE_TQ2_0 && src0->ne[1] != 32002 && src0->ne[1] != 96 && src0->ne[0] != 96) { //         int32_t* int_C = (int32_t*)malloc(1 * src0->ne[1] * sizeof(int32_t)); //         for (int i = 0; i < src0->ne[1] * 1; i++) { //             int_C[i] = 0; //         } //         float* act_scale = (float*)malloc(sizeof(float)); //         float* i2_scale = (float*)malloc(sizeof(float)); //         int32_t* int_B = (int32_t*)malloc(1 * src0->ne[0] * sizeof(int32_t)); //         int32_t* int_A = (int32_t*)malloc(src0->ne[0] * src0->ne[1] * sizeof(int32_t)); //         float_act_quant(src1->ne[0], (float*)src1->data, int_B, act_scale); //         if (src0->type == 0) { //             weight_quant_f32(src0->ne[1], src0->ne[0], src0->data, int_A, i2_scale); //         } else if (src0->type == 1) { //             weight_quant_f16(src0->ne[1], src0->ne[0], src0->data, int_A, i2_scale); //         }    //         matrixMultiply_int(src0->ne[1], src1->ne[1], src0->ne[0], int_A, int_B, int_C); //         for (int i=0; i < src0->ne[1] * 1; i++) { //             ((float*)(dst->data))[i] = int_C[i] / act_scale[0] * i2_scale[0]; //         } //         free(int_A); //         free(int_B); //         free(int_C); //         free(act_scale); //         free(i2_scale); //         return; //     } // #endif     // nb01 >= nb00 - src0 is not transposed     //   compute by src0 rows #if defined(GGML_BITNET_ARM_TL1)     if (ggml_bitnet_can_mul_mat(src0, src1, dst)) {          const int bits = ggml_bitnet_get_type_bits(type);         // src0: weight,     ne00 = k, ne01 = n         // src1: activation, ne10 = k, ne11 = m         char * wdata = params->wdata;          struct bitnet_tensor_extra * wt = src0->extra;         char * cur_wdata = wdata;         bitnet_float_type * bitnet_f_ptr = wdata;         if (sizeof(bitnet_float_type) == 2) {             cur_wdata = wdata + MAX(ne10, ne01) * ne11 * sizeof(bitnet_float_type);         };         int8_t * qlut = cur_wdata;         bitnet_float_type * lut_scales = (bitnet_float_type *) (qlut + ne10 * ne11 * 16);         bitnet_float_type * lut_biases = (bitnet_float_type *) (lut_scales + wt->lut_scales_size * ne11);          // g = 4         if (ith == 0) {             // Transform tensor if not already transformed             // Although we have done this in file `llama.cpp`,             // we still need to do it here for non-model inference, e.g., test-backend-ops.cpp.             // It's better to do this in ggml-backend.c,             // but llama.cpp directly manipulates tensor.data for cbe in a lot of space.             ggml_bitnet_transform_tensor(src0);             GGML_ASSERT(src1->type == GGML_TYPE_F32);             bitnet_float_type * act_input;             if (sizeof(bitnet_float_type) == 2) {                 ggml_fp32_to_fp16_row(src1->data, bitnet_f_ptr, ne10 * ne11);                 act_input = bitnet_f_ptr;             } else {                 act_input = src1->data;             }             ggml_preprocessor(ne01, ne10, act_input, lut_scales, qlut);         }          ggml_barrier(params->threadpool);          bitnet_float_type * act_output;         if (sizeof(bitnet_float_type) == 2) {             act_output = bitnet_f_ptr;         } else {             act_output = dst->data;         }         const int n_tile_num = wt->n_tile_num;         GGML_ASSERT(ne0 % n_tile_num == 0);         const int w_size           = ne00 * ne01 / 4;         const int w_tile_size      = w_size / n_tile_num;         const int c_size           = ne01 * ne11;         const int c_tile_size      = c_size / n_tile_num;         const int lut_size         = ne11 * 16 * (ne10 / 2) * 2; // int8         const int lut_tile_size    = lut_size / n_tile_num;          const int th_tile_num = (n_tile_num + nth - 1) / nth;         const int th_tile_beg = ith * th_tile_num;         const int th_tile_end = MIN((ith + 1) * th_tile_num, n_tile_num);          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int scales_offset     = 0;              const int qlut_offset       = 0;             const int lut_scales_offset = 0;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( ne01, ne00, ((uint8_t *)(wt->qweights) + w_offset),                              qlut,                              wt->scales + scales_offset,                              lut_scales + lut_scales_offset,                              act_output + dst_offset);             if (sizeof(bitnet_float_type) == 2) {                 ggml_fp16_to_fp32_row(act_output + dst_offset, (float *) dst->data + dst_offset, ne01 / n_tile_num);             }         }         return;     } #endif #if defined(GGML_BITNET_X86_TL2)     if (ggml_bitnet_can_mul_mat(src0, src1, dst)) {         // src0: weight,     ne00 = k, ne01 = n         // src1: activation, ne10 = k, ne11 = m         char * wdata = params->wdata;          struct bitnet_tensor_extra * wt = src0->extra;         char * cur_wdata = wdata;         bitnet_float_type * bitnet_f_ptr = wdata;         if (sizeof(bitnet_float_type) == 2) {             cur_wdata = wdata + MAX(ne10, ne01) * ne11 * sizeof(bitnet_float_type);         };         int8_t * three_qlut = cur_wdata;         bitnet_float_type * lut_scales;         int8_t * two_qlut;         const int total_k = ne10;         const int three_k = (int)(total_k / wt->BK) * wt->BK;         const int two_k = total_k - three_k;          lut_scales = (bitnet_float_type *) (three_qlut + three_k / 3 * 16 * 2 * ne11);         two_qlut = (int8_t *) (lut_scales + ne11);          // g = 4         if (ith == 0) {             // Transform tensor if not already transformed             // Although we have done this in file `llama.cpp`,             // we still need to do it here for non-model inference, e.g., test-backend-ops.cpp.             // It's better to do this in ggml-backend.c,             // but llama.cpp directly manipulates tensor.data for cbe in a lot of space.             ggml_bitnet_transform_tensor(src0);             GGML_ASSERT(src1->type == GGML_TYPE_F32);             bitnet_float_type * act_input;             if (sizeof(bitnet_float_type) == 2) {                 ggml_fp32_to_fp16_row(src1->data, bitnet_f_ptr, ne10 * ne11);                 act_input = bitnet_f_ptr;             } else {                 act_input = src1->data;             }             ggml_preprocessor(ne11, ne01, three_k, two_k, act_input, lut_scales, three_qlut, two_qlut);         }          ggml_barrier(params->threadpool);          bitnet_float_type * act_output;         if (sizeof(bitnet_float_type) == 2) {             act_output = bitnet_f_ptr;         } else {             act_output = dst->data;         }          const int n_tile_num = wt->n_tile_num;         GGML_ASSERT(ne0 % n_tile_num == 0);         const int w_size           = three_k * ne01 / (2 * 3);         const int w_tile_size      = w_size / n_tile_num;         const int c_size           = ne01;         const int c_tile_size      = c_size / n_tile_num;         const int sign_size        = three_k * ne01 / 24;         const int sign_tile_size   = sign_size / n_tile_num;          const int th_tile_num = (n_tile_num + nth - 1) / nth;         const int th_tile_beg = ith * th_tile_num;         const int th_tile_end = MIN((ith + 1) * th_tile_num, n_tile_num);          uint8_t* sign = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2;          if (ne11 > 1) {             // printf(\"ne11:%d\\n\", ne11);             int iter = ne11;             int bs512_num = iter / 512;             iter = iter - 512 * bs512_num;             int bs256_num = iter / 256;             iter = iter - 256 * bs256_num;             int bs128_num = iter / 128;             iter = iter - 128 * bs128_num;             int bs32_num = iter / 32;             iter = iter - 32 * bs32_num;             int bs8_num = iter / 8;             iter = iter - 8 * bs8_num;             int bs1_num = iter / 1;          // bs 512         for (int i = 0; i < bs512_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 512, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + i * 512 * three_k / 3 * 32,                             wt->scales,                             lut_scales + i * 512,                             act_output + dst_offset + i * 512 * ne01);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 512, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + i * 512 * two_k / 2 * 32,                             wt->scales,                             lut_scales + i * 512,                             act_output + two_dst_offset + i * 512 * ne01);         }                      }          // bs 256         for (int i = 0; i < bs256_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 256, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + bs512_num * 512 * three_k / 3 * 32 + i * 256 * three_k / 3 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + i * 256,                             act_output + dst_offset + bs512_num * 512 * ne01 + i * 256 * ne01);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 256, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + bs512_num * 512 * two_k / 2 * 32 + i * 256 * two_k / 2 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + i * 256,                             act_output + two_dst_offset + bs512_num * 512 * ne01 + i * 256 * ne01);         }                      }          // bs 128         // printf(\"128:%d\\n\", bs128_num);         for (int i = 0; i < bs128_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 128, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + bs512_num * 512 * three_k / 3 * 32 + bs256_num * 256 * three_k / 3 * 32 + i * 128 * three_k / 3 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256 + i * 128,                             act_output + dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01 + i * 128 * ne01);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 128, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + bs512_num * 512 * two_k / 2 * 32 + bs256_num * 256 * two_k / 2 * 32 + i * 128 * two_k / 2 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256 + i * 128,                             act_output + two_dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01 + i * 128 * ne01);         }                      }         // printf(\"128end\\n\");          // bs 32         // printf(\"32:%d\\n\", bs32_num);         for (int i = 0; i < bs32_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 32, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + bs512_num * 512 * three_k / 3 * 32 + bs256_num * 256 * three_k / 3 * 32\\                              + bs128_num * 128 * three_k / 3 * 32 + i * 32 * three_k / 3 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256 + bs128_num * 128 + i * 32,                             act_output + dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01 + bs128_num * 128 * ne01 + i * 32 * ne01);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 32, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + bs512_num * 512 * two_k / 3 * 32 + bs256_num * 256 * two_k / 3 * 32\\                              + bs128_num * 128 * two_k / 2 * 32 + i * 32 * two_k / 2 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256 + bs128_num * 128 + i * 32,                             act_output + two_dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01 + bs128_num * 128 * ne01 + i * 32 * ne01);         }                      }         // printf(\"32end\\n\");             // bs 8         // printf(\"8:%d\\n\", bs8_num);         for (int i = 0; i < bs8_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 8, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + bs512_num * 512 * three_k / 3 * 32 + bs256_num * 256 * three_k / 3 * 32\\                              + bs128_num * 128 * three_k / 3 * 32 + bs32_num * 32 * three_k / 3 * 32\\                             + i * 8 * three_k / 3 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256\\                              + bs128_num * 128 + bs32_num * 32 + i * 8,                             act_output + dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01\\                              + bs128_num * 128 * ne01 + bs32_num * 32 * ne01 + \\                             i * 8 * ne01);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 8, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + bs512_num * 512 * two_k / 3 * 32 + bs256_num * 256 * two_k / 3 * 32\\                              + bs128_num * 128 * two_k / 2 * 32 + bs32_num * 32 * two_k / 2 * 32\\                              + i * 8 * two_k / 2 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256\\                              + bs128_num * 128 + bs32_num * 32 + i * 8,                             act_output + two_dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01\\                              + bs128_num * 128 * ne01 + bs32_num * 32 * ne01\\                              + i * 8 * ne01);         }                      }         // printf(\"8end\\n\");          // bs 1         // printf(\"1:%d\\n\", bs1_num);         for (int i = 0; i < bs1_num; i++) {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 1, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut + bs512_num * 512 * three_k / 3 * 32 + bs256_num * 256 * three_k / 3 * 32 + \\                             bs128_num * 128 * three_k / 3 * 32 + bs32_num * 32 * three_k / 3 * 32\\                             + bs8_num * 8 * three_k / 3 * 32 + i * 1 * three_k / 3 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256\\                              + bs128_num * 128 + bs32_num * 32 + bs8_num * 8 + i * 1,                             act_output + dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01\\                              + bs128_num * 128 * ne01 + bs32_num * 32 * ne01 + \\                             bs8_num * 8 * ne01 + i * 1 * ne01);         }                   const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 1, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut + bs512_num * 512 * two_k / 3 * 32 + bs256_num * 256 * two_k / 3 * 32\\                              + bs128_num * 128 * two_k / 2 * 32 + bs32_num * 32 * two_k / 2 * 32\\                              + bs8_num * 8 * two_k / 2 * 32 + i * 1 * two_k / 2 * 32,                             wt->scales,                             lut_scales + bs512_num * 512 + bs256_num * 256\\                              + bs128_num * 128 + bs32_num * 32 + bs8_num * 8 + i * 1,                             act_output + two_dst_offset + bs512_num * 512 * ne01 + bs256_num * 256 * ne01\\                              + bs128_num * 128 * ne01 + bs32_num * 32 * ne01\\                              + bs8_num * 8 * ne01 + i * 1 * ne01);         }                      }         // printf(\"1end\\n\");          } else {          for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int w_offset          = i_tile * w_tile_size;             const int sign_offset       = i_tile * sign_tile_size;             const int dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 1, ne01, ne00, three_k, ((uint8_t *)(wt->qweights) + w_offset),                             sign + sign_offset,                             three_qlut,                             wt->scales,                             lut_scales,                             act_output + dst_offset);         }          const int two_w_size           = ne01 * two_k / (2 * 2); // int8          const int two_w_tile_size      = two_w_size / n_tile_num;         uint8_t* two_A = ((uint8_t *)(wt->qweights)) + three_k * ne01 / 3 / 2 + three_k * ne01 / 3 / 8;         // auto gemm_start = std::chrono::high_resolution_clock::now();         for (int i_tile = th_tile_beg; i_tile < th_tile_end; i_tile++) {             const int two_w_offset          = i_tile * two_w_tile_size;             const int two_dst_offset        = i_tile * c_tile_size;              ggml_qgemm_lut( 1, ne01, ne00, two_k, two_A + two_w_offset,                              NULL,                             two_qlut,                             wt->scales,                             lut_scales,                             act_output + two_dst_offset);         }                   }             return;     } #endif  #if GGML_USE_LLAMAFILE     // broadcast factors     const int64_t r2 = ne12 / ne02;     const int64_t r3 = ne13 / ne03;      const bool src1_cont = ggml_is_contiguous(src1);      if (src1_cont) {         for (int64_t i13 = 0; i13 < ne13; i13++)             for (int64_t i12 = 0; i12 < ne12; i12++)                 if (!llamafile_sgemm(ne01, ne11, ne00/ggml_blck_size(src0->type),                                      (const char *)src0->data + i12/r2*nb02 + i13/r3*nb03,                                      nb01/ggml_type_size(src0->type),                                      (const char *)src1->data + i12*nb12 + i13*nb13,                                      nb11/ggml_type_size(src1->type),                                      (char *)dst->data + i12*nb2 + i13*nb3,                                      nb1/ggml_type_size(dst->type),                                      ith, nth,                                      src0->type,                                      src1->type,                                      dst->type))                     goto UseGgmlGemm1;         return;     } UseGgmlGemm1:; #endif      if (src1->type != vec_dot_type) {         char * wdata = params->wdata;          const size_t nbw1 = ggml_row_size(vec_dot_type, ne10);         const size_t nbw2 = nbw1*ne11;         const size_t nbw3 = nbw2*ne12;          assert(params->wsize >= ne13*nbw3);         GGML_ASSERT(src1->type == GGML_TYPE_F32);          float* act_scales = (float*) ((char *) wdata + (ne11 * ne10));         int32_t* act_sums = (int32_t*) ((char *) act_scales + (ne11) * sizeof(float));          for (int64_t i13 = 0; i13 < ne13; ++i13) {             for (int64_t i12 = 0; i12 < ne12; ++i12) {                 int64_t i11_processed = 0;                 if ((ggml_n_dims(src1) == 2) && from_float_to_mat && gemm) {                     for (int64_t i11 = ith * 4; i11 < ne11 - ne11 % 4; i11 += nth * 4) {                         from_float_to_mat((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11),                                           (void *)               (wdata + i13*nbw3 + i12*nbw2 + i11*nbw1),                                           4, ne10, blck_size_interleave);                     }                     i11_processed = ne11 - ne11 % 4;                 }                 for (int64_t i11 = i11_processed + ith; i11 < ne11; i11 += nth) {                     if (src0->type == GGML_TYPE_I2_S) {                         quantize_row_i8_s((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) (wdata + i13*nbw3 + i12*nbw2 + i11*nbw1), ne10, act_scales + i11, act_sums + i11);                     } else {                         from_float((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11),                         (void *)               (wdata + i13*nbw3 + i12*nbw2 + i11*nbw1),                         ne10);                     }                 }             }         }     }      if (ith == 0) {         // Every thread starts at ith, so the first unprocessed chunk is nth.  This save a bit of coordination right at the start.         atomic_store_explicit(&params->threadpool->current_chunk, nth, memory_order_relaxed);     }      ggml_barrier(params->threadpool);  #if GGML_USE_LLAMAFILE     if (src1->type != vec_dot_type) {         const void* wdata = (src1->type == vec_dot_type) ? src1->data : params->wdata;         const size_t row_size = ggml_row_size(vec_dot_type, ne10);          for (int64_t i13 = 0; i13 < ne13; i13++)             for (int64_t i12 = 0; i12 < ne12; i12++)                 if (!llamafile_sgemm(ne01, ne11, ne00/ggml_blck_size(src0->type),                                      (const char *)src0->data + i12/r2*nb02 + i13/r3*nb03,                                      nb01/ggml_type_size(src0->type),                                      (const char *)wdata + (i12*ne11 + i13*ne12*ne11)*row_size,                                      row_size/ggml_type_size(vec_dot_type),                                      (char *)dst->data + i12*nb2 + i13*nb3,                                      nb1/ggml_type_size(dst->type),                                      ith, nth,                                      src0->type,                                      vec_dot_type,                                      dst->type))                     goto UseGgmlGemm2;         return;     } UseGgmlGemm2:; #endif      // This is the size of the first dimension of the result, so we can iterate that way. (see the ASSERT above, these are the same numbers)     const int64_t nr0 = ne0;      // This is the size of the rest of the dimensions of the result     const int64_t nr1 = ne1 * ne2 * ne3;      // dot kernels can handle 1 row and col at a time, but mmla kernels can process 2 rows and cols     int64_t num_rows_per_vec_dot = vec_dot_num_rows;     // TODO: currently the mmla kernels support only even numbered rows/cols.     // this check can be removed once they are extended to support odd numbered rows/cols too     if ((nr0 % 2 != 0) || (ne11 % 2 != 0)) {         num_rows_per_vec_dot = 1;     }      // Now select a reasonable chunk size.     int chunk_size = 16;      // We need to step up the size if it's small     if (nr0 == 1 || nr1 == 1) {         chunk_size = 64;     }      // distribute the work across the inner or outer loop based on which one is larger     // The number of chunks in the 0/1 dim.     // CEIL(nr0/chunk_size)     int64_t nchunk0 = (nr0 + chunk_size - 1) / chunk_size;     int64_t nchunk1 = (nr1 + chunk_size - 1) / chunk_size;      // If the chunking is poor for the number of threads on this setup, scrap the whole plan.  Re-chunk it by thread.     //   Also, chunking by thread was measured to have perform better on NUMA systems.  See https://github.com/ggerganov/llama.cpp/pull/6915     //   In theory, chunking should be just as useful on NUMA and non NUMA systems, but testing disagreed with that.     if (nchunk0 * nchunk1 < nth * 4 || ggml_is_numa()) {         // distribute the thread work across the inner or outer loop based on which one is larger         nchunk0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows         nchunk1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows     }      // The number of elements in each chunk     const int64_t dr0 = (nr0 + nchunk0 - 1) / nchunk0;     const int64_t dr1 = (nr1 + nchunk1 - 1) / nchunk1;      if ((ggml_n_dims(src0) == 2) && gemv) {         const void * src1_wdata      = (src1->type == vec_dot_type) ? src1->data : params->wdata;         const size_t src1_col_stride = ggml_is_contiguous(src1) || src1->type != vec_dot_type ? ggml_row_size(vec_dot_type, ne10) : nb11;         int64_t src0_start = (ith * ne01) / nth;         int64_t src0_end   = ((ith + 1) * ne01) / nth;         src0_start = (src0_start % matmul_num_cols) ? src0_start + matmul_num_cols - (src0_start % matmul_num_cols): src0_start;         src0_end   = (src0_end   % matmul_num_cols) ? src0_end   + matmul_num_cols - (src0_end   % matmul_num_cols): src0_end;         if (src0_start >= src0_end) return;          // If there are more than three rows in src1, use gemm; otherwise, use gemv.         if (gemm && (ne11 > 3)) {             gemm(ne00, (float *)((char *) dst->data) + src0_start, ne01, (const char *) src0->data + src0_start * nb01,                  (const char *) src1_wdata, ne11 - ne11 % 4, src0_end - src0_start);         }         for (int iter = gemm ? ne11 - ne11 % 4 : 0; iter < ne11; iter++) {             gemv(ne00, (float *)((char *) dst->data + (iter * nb1)) + src0_start, ne01,                  (const char *) src0->data + src0_start * nb01, (const char *) src1_wdata + (src1_col_stride * iter), 1,                  src0_end - src0_start);         }         return;     }      // The first chunk comes from our thread_id, the rest will get auto-assigned.     int current_chunk = ith;      while (current_chunk < nchunk0 * nchunk1) {         const int64_t ith0 = current_chunk % nchunk0;         const int64_t ith1 = current_chunk / nchunk0;          const int64_t ir0_start = dr0 * ith0;         const int64_t ir0_end = MIN(ir0_start + dr0, nr0);          const int64_t ir1_start = dr1 * ith1;         const int64_t ir1_end = MIN(ir1_start + dr1, nr1);          ggml_compute_forward_mul_mat_one_chunk(params, dst, num_rows_per_vec_dot, ir0_start, ir0_end, ir1_start, ir1_end);          if (nth >= nchunk0 * nchunk1) {             break;         }          current_chunk = atomic_fetch_add_explicit(&params->threadpool->current_chunk, 1, memory_order_relaxed);     } }",
      "construct_id": "8ed9280e-5342-4800-91a1-0e9e16f1a1fa",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.237253",
        "updated_at": "2025-06-12T17:27:13.237265",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 12582,
          "end": 13263
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER",
          "RANK 5"
        ]
      }
    },
    {
      "id": "fefa3443-c502-407e-8bf8-330045d1c380",
      "content": "/////////////////////////////////  static void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {     GGML_ASSERT(params);      if (tensor->op == GGML_OP_NONE || ggml_is_empty(tensor)) {         return;     }      switch (tensor->op) {         case GGML_OP_DUP:             {                 ggml_compute_forward_dup(params, tensor);             } break;         case GGML_OP_ADD:             {                 ggml_compute_forward_add(params, tensor);             } break;         case GGML_OP_ADD1:             {                 ggml_compute_forward_add1(params, tensor);             } break;         case GGML_OP_ACC:             {                 ggml_compute_forward_acc(params, tensor);             } break;         case GGML_OP_SUB:             {                 ggml_compute_forward_sub(params, tensor);             } break;         case GGML_OP_MUL:             {                 ggml_compute_forward_mul(params, tensor);             } break;         case GGML_OP_DIV:             {                 ggml_compute_forward_div(params, tensor);             } break;         case GGML_OP_SQR:             {                 ggml_compute_forward_sqr(params, tensor);             } break;         case GGML_OP_SQRT:             {                 ggml_compute_forward_sqrt(params, tensor);             } break;         case GGML_OP_LOG:             {                 ggml_compute_forward_log(params, tensor);             } break;         case GGML_OP_SIN:             {                 ggml_compute_forward_sin(params, tensor);             } break;         case GGML_OP_COS:             {                 ggml_compute_forward_cos(params, tensor);             } break;         case GGML_OP_SUM:             {                 ggml_compute_forward_sum(params, tensor);             } break;         case GGML_OP_SUM_ROWS:             {                 ggml_compute_forward_sum_rows(params, tensor);             } break;         case GGML_OP_MEAN:             {                 ggml_compute_forward_mean(params, tensor);             } break;         case GGML_OP_ARGMAX:             {                 ggml_compute_forward_argmax(params, tensor);             } break;         case GGML_OP_COUNT_EQUAL:             {                 ggml_compute_forward_count_equal(params, tensor);             } break;         case GGML_OP_REPEAT:             {                 ggml_compute_forward_repeat(params, tensor);             } break;         case GGML_OP_REPEAT_BACK:             {                 ggml_compute_forward_repeat_back(params, tensor);             } break;         case GGML_OP_CONCAT:             {                 ggml_compute_forward_concat(params, tensor);             } break;         case GGML_OP_SILU_BACK:             {                 ggml_compute_forward_silu_back(params, tensor);             } break;         case GGML_OP_NORM:             {                 ggml_compute_forward_norm(params, tensor);             } break;         case GGML_OP_RMS_NORM:             {                 ggml_compute_forward_rms_norm(params, tensor);             } break;         case GGML_OP_RMS_NORM_BACK:             {                 ggml_compute_forward_rms_norm_back(params, tensor);             } break;         case GGML_OP_GROUP_NORM:             {                 ggml_compute_forward_group_norm(params, tensor);             } break;         case GGML_OP_MUL_MAT:             {                 ggml_compute_forward_mul_mat(params, tensor);             } break;         case GGML_OP_MUL_MAT_ID:             {                 ggml_compute_forward_mul_mat_id(params, tensor);             } break;         case GGML_OP_OUT_PROD:             {                 ggml_compute_forward_out_prod(params, tensor);             } break;         case GGML_OP_SCALE:             {                 ggml_compute_forward_scale(params, tensor);             } break;         case GGML_OP_SET:             {                 ggml_compute_forward_set(params, tensor);             } break;         case GGML_OP_CPY:             {                 ggml_compute_forward_cpy(params, tensor);             } break;         case GGML_OP_CONT:             {                 ggml_compute_forward_cont(params, tensor);             } break;         case GGML_OP_RESHAPE:             {                 ggml_compute_forward_reshape(params, tensor);             } break;         case GGML_OP_VIEW:             {                 ggml_compute_forward_view(params, tensor);             } break;         case GGML_OP_PERMUTE:             {                 ggml_compute_forward_permute(params, tensor);             } break;         case GGML_OP_TRANSPOSE:             {                 ggml_compute_forward_transpose(params, tensor);             } break;         case GGML_OP_GET_ROWS:             {                 ggml_compute_forward_get_rows(params, tensor);             } break;         case GGML_OP_GET_ROWS_BACK:             {                 ggml_compute_forward_get_rows_back(params, tensor);             } break;         case GGML_OP_DIAG:             {                 ggml_compute_forward_diag(params, tensor);             } break;         case GGML_OP_DIAG_MASK_INF:             {                 ggml_compute_forward_diag_mask_inf(params, tensor);             } break;         case GGML_OP_DIAG_MASK_ZERO:             {                 ggml_compute_forward_diag_mask_zero(params, tensor);             } break;         case GGML_OP_SOFT_MAX:             {                 ggml_compute_forward_soft_max(params, tensor);             } break;         case GGML_OP_SOFT_MAX_BACK:             {                 ggml_compute_forward_soft_max_back(params, tensor);             } break;         case GGML_OP_ROPE:             {                 ggml_compute_forward_rope(params, tensor);             } break;         case GGML_OP_ROPE_BACK:             {                 ggml_compute_forward_rope_back(params, tensor);             } break;         case GGML_OP_CLAMP:             {                 ggml_compute_forward_clamp(params, tensor);             } break;         case GGML_OP_CONV_TRANSPOSE_1D:             {                 ggml_compute_forward_conv_transpose_1d(params, tensor);             } break;         case GGML_OP_IM2COL:             {                 ggml_compute_forward_im2col(params, tensor);             } break;         case GGML_OP_IM2COL_BACK:             {                 ggml_compute_forward_im2col_back_f32(params, tensor);             } break;         case GGML_OP_CONV_TRANSPOSE_2D:             {                 ggml_compute_forward_conv_transpose_2d(params, tensor);             } break;         case GGML_OP_POOL_1D:             {                 ggml_compute_forward_pool_1d(params, tensor);             } break;         case GGML_OP_POOL_2D:             {                 ggml_compute_forward_pool_2d(params, tensor);             } break;         case GGML_OP_POOL_2D_BACK:             {                 ggml_compute_forward_pool_2d_back(params, tensor);             } break;         case GGML_OP_UPSCALE:             {                 ggml_compute_forward_upscale(params, tensor);             } break;         case GGML_OP_PAD:             {                 ggml_compute_forward_pad(params, tensor);             } break;         case GGML_OP_ARANGE:             {                 ggml_compute_forward_arange(params, tensor);             } break;         case GGML_OP_TIMESTEP_EMBEDDING:             {                 ggml_compute_forward_timestep_embedding(params, tensor);             } break;         case GGML_OP_ARGSORT:             {                 ggml_compute_forward_argsort(params, tensor);             } break;         case GGML_OP_LEAKY_RELU:             {                 ggml_compute_forward_leaky_relu(params, tensor);             } break;         case GGML_OP_FLASH_ATTN_EXT:             {                 ggml_compute_forward_flash_attn_ext(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor->src[3], tensor);             } break;         case GGML_OP_FLASH_ATTN_BACK:             {                 int32_t t = ggml_get_op_params_i32(tensor, 0);                 GGML_ASSERT(t == 0 || t == 1);                 bool masked = t != 0;                 ggml_compute_forward_flash_attn_back(params, masked, tensor);             } break;         case GGML_OP_SSM_CONV:             {                 ggml_compute_forward_ssm_conv(params, tensor);             } break;         case GGML_OP_SSM_SCAN:             {                 ggml_compute_forward_ssm_scan(params, tensor);             } break;         case GGML_OP_WIN_PART:             {                 ggml_compute_forward_win_part(params, tensor);             } break;         case GGML_OP_WIN_UNPART:             {                 ggml_compute_forward_win_unpart(params, tensor);             } break;         case GGML_OP_UNARY:             {                 ggml_compute_forward_unary(params, tensor);             } break;         case GGML_OP_GET_REL_POS:             {                 ggml_compute_forward_get_rel_pos(params, tensor);             } break;         case GGML_OP_ADD_REL_POS:             {                 ggml_compute_forward_add_rel_pos(params, tensor);             } break;         case GGML_OP_RWKV_WKV:             {                 ggml_compute_forward_rwkv_wkv(params, tensor);             } break;         case GGML_OP_MAP_UNARY:             {                 ggml_unary_op_f32_t fun;                 memcpy(&fun, tensor->op_params, sizeof(fun));                 ggml_compute_forward_map_unary(params, tensor, fun);             }             break;         case GGML_OP_MAP_BINARY:             {                 ggml_binary_op_f32_t fun;                 memcpy(&fun, tensor->op_params, sizeof(fun));                 ggml_compute_forward_map_binary(params, tensor, fun);             }             break;         case GGML_OP_MAP_CUSTOM1_F32:             {                 ggml_custom1_op_f32_t fun;                 memcpy(&fun, tensor->op_params, sizeof(fun));                 ggml_compute_forward_map_custom1_f32(params, tensor, fun);             }             break;         case GGML_OP_MAP_CUSTOM2_F32:             {                 ggml_custom2_op_f32_t fun;                 memcpy(&fun, tensor->op_params, sizeof(fun));                 ggml_compute_forward_map_custom2_f32(params, tensor, fun);             }             break;         case GGML_OP_MAP_CUSTOM3_F32:             {                 ggml_custom3_op_f32_t fun;                 memcpy(&fun, tensor->op_params, sizeof(fun));                 ggml_compute_forward_map_custom3_f32(params, tensor, fun);             }             break;         case GGML_OP_MAP_CUSTOM1:             {                 ggml_compute_forward_map_custom1(params, tensor);             }             break;         case GGML_OP_MAP_CUSTOM2:             {                 ggml_compute_forward_map_custom2(params, tensor);             }             break;         case GGML_OP_MAP_CUSTOM3:             {                 ggml_compute_forward_map_custom3(params, tensor);             }             break;         case GGML_OP_CROSS_ENTROPY_LOSS:             {                 ggml_compute_forward_cross_entropy_loss(params, tensor);             }             break;         case GGML_OP_CROSS_ENTROPY_LOSS_BACK:             {                 ggml_compute_forward_cross_entropy_loss_back(params, tensor);             }             break;         case GGML_OP_OPT_STEP_ADAMW:             {                 ggml_compute_forward_opt_step_adamw(params, tensor);             }             break;         case GGML_OP_NONE:             {                 // nop             } break;         case GGML_OP_COUNT:             {                 GGML_ABORT(\"fatal error\");             }     } }",
      "construct_id": "7ae24fc7-87f9-49bc-8d84-e6c46c31928e",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.238667",
        "updated_at": "2025-06-12T17:27:13.238681",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 17830,
          "end": 18193
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER",
          "RANK 6"
        ]
      }
    },
    {
      "id": "c801cd4d-9b25-4fc0-bd58-bcaa225f5804",
      "content": "size_t ggml_type_size(enum ggml_type type) {     return type_traits[type].type_size; }",
      "construct_id": "3a137da2-31e7-413b-ac00-8eecbd731a13",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.239470",
        "updated_at": "2025-06-12T17:27:13.239483",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 3507,
          "end": 3509
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER",
          "RANK 7"
        ]
      }
    },
    {
      "id": "b2c6553e-d139-4c1c-b5a5-19714143cd41",
      "content": "// static inline int nearest_int(float fval) {     assert(fabsf(fval) <= 4194303.f);     float val = fval + 12582912.f;     int i; memcpy(&i, &val, sizeof(int));     return (i & 0x007fffff) - 0x00400000; }",
      "construct_id": "964ede2c-075f-4629-8a81-52bf163e5d94",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.240220",
        "updated_at": "2025-06-12T17:27:13.240233",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml-quants.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 1638,
          "end": 1644
        },
        "tokens": null,
        "construct_tags": [
          "RANK 8",
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER"
        ]
      }
    },
    {
      "id": "79061165-7be7-4743-adf0-7eb2d8bf4f79",
      "content": "int64_t ggml_blck_size(enum ggml_type type) {     return type_traits[type].blck_size; }",
      "construct_id": "60ad4efd-5fc2-457f-a2c3-4a260eb6078c",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.240956",
        "updated_at": "2025-06-12T17:27:13.240968",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 3503,
          "end": 3505
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER",
          "RANK 9"
        ]
      }
    },
    {
      "id": "434b25b5-ba24-4d71-b325-7a4f67413867",
      "content": "static bool ggml_is_contiguous_n(const struct ggml_tensor * tensor, int n) {     size_t next_nb = ggml_type_size(tensor->type);     if (tensor->ne[0] != ggml_blck_size(tensor->type) && tensor->nb[0] != next_nb) {         return false;     }     next_nb *= tensor->ne[0]/ggml_blck_size(tensor->type);     for (int i = 1; i < GGML_MAX_DIMS; i++) {         if (tensor->ne[i] != 1) {             if (i > n) {                 if (tensor->nb[i] != next_nb) {                     return false;                 }                 next_nb *= tensor->ne[i];             } else {                 // this dimension does not need to be contiguous                 next_nb = tensor->ne[i]*tensor->nb[i];             }         }     }     return true; }",
      "construct_id": "a4a1d71a-7ffd-4aff-9763-9966bebc7fe9",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-12T17:27:13.241826",
        "updated_at": "2025-06-12T17:27:13.241839",
        "language": "c",
        "file": "3rdparty/llama.cpp/ggml/src/ggml.c",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 3645,
          "end": 3665
        },
        "tokens": null,
        "construct_tags": [
          "RANK 10",
          "RANK 1-10",
          "FUNCTION",
          "VTUNE",
          "PROFILER"
        ]
      }
    }
  ]
}