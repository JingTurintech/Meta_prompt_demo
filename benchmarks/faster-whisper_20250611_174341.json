{
  "metadata": {
    "collected_at": "20250611_174341",
    "project_info": {
      "project_id": "9f8f7777-f359-4f39-bfa8-6a0f4ebe473c",
      "name": "faster-whisper",
      "description": "https://github.com/SYSTRAN/faster-whisper/pull/856/commits",
      "language": "py"
    }
  },
  "code_snippets": [
    {
      "id": "64c81c55-a1bb-4738-a69d-a65bf60d5866",
      "content": "def inference_fast():     \"\"\"Fast inference for testing - single run only\"\"\"     benchmark_dir = os.path.dirname(__file__)     audio_path = os.path.join(benchmark_dir, \"benchmark.m4a\")          # Just do one transcription, no iteration     segments, info = model.transcribe(audio_path, language=\"en\")     total_segments = 0     for segment in segments:         print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))         total_segments += 1          print(f\"Processed {total_segments} segments\")     return total_segments",
      "construct_id": "31775e6f-eae5-4839-8ed8-9ac26d587b86",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.524075",
        "updated_at": "2025-06-11T16:24:14.524092",
        "language": "py",
        "file": "benchmark/speed_benchmark_fast.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 13,
          "end": 26
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1",
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE"
        ]
      }
    },
    {
      "id": "63daabe6-32b4-4193-88e0-3a611521ee81",
      "content": "def measure_speed_fast(func: Callable[[], None]):     \"\"\"Run benchmark multiple times and calculate statistics\"\"\"     import time          print(f\"Starting fast benchmark with {args.repeat} repetitions...\")     execution_times = []          for i in range(args.repeat):         print(f\"\\n--- Run {i+1}/{args.repeat} ---\")         start_time = time.time()         result = func()         end_time = time.time()                  duration = end_time - start_time         execution_times.append(duration)         print(f\"Run {i+1} execution time: {duration:.3f}s\")          # Calculate statistics     min_time = min(execution_times)     max_time = max(execution_times)     mean_time = statistics.mean(execution_times)     median_time = statistics.median(execution_times)          # Calculate standard deviation if we have more than 1 run     if len(execution_times) > 1:         std_dev = statistics.stdev(execution_times)         print(f\"\\n=== BENCHMARK RESULTS ({args.repeat} runs) ===\")         print(f\"Min time:    {min_time:.3f}s\")         print(f\"Max time:    {max_time:.3f}s\")          print(f\"Mean time:   {mean_time:.3f}s\")         print(f\"Median time: {median_time:.3f}s\")         print(f\"Std dev:     {std_dev:.3f}s\")         print(f\"All times:   {[f'{t:.3f}s' for t in execution_times]}\")     else:         print(f\"\\n=== BENCHMARK RESULTS ===\")         print(f\"Execution time: {execution_times[0]:.3f}s\")          return result",
      "construct_id": "9349e528-2497-4e0a-b458-7011637e63c4",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.525050",
        "updated_at": "2025-06-11T16:24:14.525065",
        "language": "py",
        "file": "benchmark/speed_benchmark_fast.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 37,
          "end": 74
        },
        "tokens": null,
        "construct_tags": [
          "RANK 2",
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE"
        ]
      }
    },
    {
      "id": "66c550b4-e370-4641-9bae-893df183d2ad",
      "content": "    def generate_segments(         self,         features: np.ndarray,         tokenizer: Tokenizer,         options: TranscriptionOptions,         encoder_output: Optional[ctranslate2.StorageView] = None,     ) -> Iterable[Segment]:         content_frames = features.shape[-1] - self.feature_extractor.nb_max_frames         content_duration = float(content_frames * self.feature_extractor.time_per_frame)          if isinstance(options.clip_timestamps, str):             options = options._replace(                 clip_timestamps=[                     float(ts)                     for ts in (                         options.clip_timestamps.split(\",\")                         if options.clip_timestamps                         else []                     )                 ]             )         seek_points: List[int] = [             round(ts * self.frames_per_second) for ts in options.clip_timestamps         ]         if len(seek_points) == 0:             seek_points.append(0)         if len(seek_points) % 2 == 1:             seek_points.append(content_frames)         seek_clips: List[Tuple[int, int]] = list(             zip(seek_points[::2], seek_points[1::2])         )          punctuation = \"\\\"'\"\u00bf([{-\\\"'.\u3002,,!!??::\")]}\u3001\"          idx = 0         clip_idx = 0         seek = seek_clips[clip_idx][0]         all_tokens = []         prompt_reset_since = 0          if options.initial_prompt is not None:             if isinstance(options.initial_prompt, str):                 initial_prompt = \" \" + options.initial_prompt.strip()                 initial_prompt_tokens = tokenizer.encode(initial_prompt)                 all_tokens.extend(initial_prompt_tokens)             else:                 all_tokens.extend(options.initial_prompt)          last_speech_timestamp = 0.0         # NOTE: This loop is obscurely flattened to make the diff readable.         # A later commit should turn this into a simpler nested loop.         # for seek_clip_start, seek_clip_end in seek_clips:         #     while seek < seek_clip_end         while clip_idx < len(seek_clips):             seek_clip_start, seek_clip_end = seek_clips[clip_idx]             if seek_clip_end > content_frames:                 seek_clip_end = content_frames             if seek < seek_clip_start:                 seek = seek_clip_start             if seek >= seek_clip_end:                 clip_idx += 1                 if clip_idx < len(seek_clips):                     seek = seek_clips[clip_idx][0]                 continue             time_offset = seek * self.feature_extractor.time_per_frame             window_end_time = float(                 (seek + self.feature_extractor.nb_max_frames)                 * self.feature_extractor.time_per_frame             )             segment_size = min(                 self.feature_extractor.nb_max_frames,                 content_frames - seek,                 seek_clip_end - seek,             )             segment = features[:, seek : seek + segment_size]             segment_duration = segment_size * self.feature_extractor.time_per_frame             segment = pad_or_trim(segment, self.feature_extractor.nb_max_frames)              if self.logger.isEnabledFor(logging.DEBUG):                 self.logger.debug(                     \"Processing segment at %s\", format_timestamp(time_offset)                 )              previous_tokens = all_tokens[prompt_reset_since:]             prompt = self.get_prompt(                 tokenizer,                 previous_tokens,                 without_timestamps=options.without_timestamps,                 prefix=options.prefix if seek == 0 else None,                 hotwords=options.hotwords,             )              if seek > 0 or encoder_output is None:                 encoder_output = self.encode(segment)              (                 result,                 avg_logprob,                 temperature,                 compression_ratio,             ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options)              if options.no_speech_threshold is not None:                 # no voice activity check                 should_skip = result.no_speech_prob > options.no_speech_threshold                  if (                     options.log_prob_threshold is not None                     and avg_logprob > options.log_prob_threshold                 ):                     # don't skip if the logprob is high enough, despite the no_speech_prob                     should_skip = False                  if should_skip:                     self.logger.debug(                         \"No speech threshold is met (%f > %f)\",                         result.no_speech_prob,                         options.no_speech_threshold,                     )                      # fast-forward to the next segment boundary                     seek += segment_size                     continue              tokens = result.sequences_ids[0]              previous_seek = seek             current_segments = []              # anomalous words are very long/short/improbable             def word_anomaly_score(word: dict) -> float:                 probability = word.get(\"probability\", 0.0)                 duration = word[\"end\"] - word[\"start\"]                 score = 0.0                 if probability < 0.15:                     score += 1.0                 if duration < 0.133:                     score += (0.133 - duration) * 15                 if duration > 2.0:                     score += duration - 2.0                 return score              def is_segment_anomaly(segment: Optional[dict]) -> bool:                 if segment is None or not segment[\"words\"]:                     return False                 words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]                 words = words[:8]                 score = sum(word_anomaly_score(w) for w in words)                 return score >= 3 or score + 0.01 >= len(words)              def next_words_segment(segments: List[dict]) -> Optional[dict]:                 return next((s for s in segments if s[\"words\"]), None)              single_timestamp_ending = (                 len(tokens) >= 2                 and tokens[-2] < tokenizer.timestamp_begin <= tokens[-1]             )              consecutive_timestamps = [                 i                 for i in range(len(tokens))                 if i > 0                 and tokens[i] >= tokenizer.timestamp_begin                 and tokens[i - 1] >= tokenizer.timestamp_begin             ]              if len(consecutive_timestamps) > 0:                 slices = list(consecutive_timestamps)                 if single_timestamp_ending:                     slices.append(len(tokens))                  last_slice = 0                 for current_slice in slices:                     sliced_tokens = tokens[last_slice:current_slice]                     start_timestamp_position = (                         sliced_tokens[0] - tokenizer.timestamp_begin                     )                     end_timestamp_position = (                         sliced_tokens[-1] - tokenizer.timestamp_begin                     )                     start_time = (                         time_offset + start_timestamp_position * self.time_precision                     )                     end_time = (                         time_offset + end_timestamp_position * self.time_precision                     )                      current_segments.append(                         dict(                             seek=seek,                             start=start_time,                             end=end_time,                             tokens=sliced_tokens,                         )                     )                     last_slice = current_slice                  if single_timestamp_ending:                     # single timestamp at the end means no speech after the last timestamp.                     seek += segment_size                 else:                     # otherwise, ignore the unfinished segment and seek to the last timestamp                     last_timestamp_position = (                         tokens[last_slice - 1] - tokenizer.timestamp_begin                     )                     seek += last_timestamp_position * self.input_stride              else:                 duration = segment_duration                 timestamps = [                     token for token in tokens if token >= tokenizer.timestamp_begin                 ]                 if len(timestamps) > 0 and timestamps[-1] != tokenizer.timestamp_begin:                     last_timestamp_position = timestamps[-1] - tokenizer.timestamp_begin                     duration = last_timestamp_position * self.time_precision                  current_segments.append(                     dict(                         seek=seek,                         start=time_offset,                         end=time_offset + duration,                         tokens=tokens,                     )                 )                  seek += segment_size              if options.word_timestamps:                 self.add_word_timestamps(                     current_segments,                     tokenizer,                     encoder_output,                     segment_size,                     options.prepend_punctuations,                     options.append_punctuations,                     last_speech_timestamp=last_speech_timestamp,                 )                  if not single_timestamp_ending:                     last_word_end = get_end(current_segments)                     if last_word_end is not None and last_word_end > time_offset:                         seek = round(last_word_end * self.frames_per_second)                  # skip silence before possible hallucinations                 if options.hallucination_silence_threshold is not None:                     threshold = options.hallucination_silence_threshold                      # if first segment might be a hallucination, skip leading silence                     first_segment = next_words_segment(current_segments)                     if first_segment is not None and is_segment_anomaly(first_segment):                         gap = first_segment[\"start\"] - time_offset                         if gap > threshold:                             seek = previous_seek + round(gap * self.frames_per_second)                             continue                      # skip silence before any possible hallucination that is surrounded                     # by silence or more hallucinations                     hal_last_end = last_speech_timestamp                     for si in range(len(current_segments)):                         segment = current_segments[si]                         if not segment[\"words\"]:                             continue                         if is_segment_anomaly(segment):                             next_segment = next_words_segment(                                 current_segments[si + 1 :]                             )                             if next_segment is not None:                                 hal_next_start = next_segment[\"words\"][0][\"start\"]                             else:                                 hal_next_start = time_offset + segment_duration                             silence_before = (                                 segment[\"start\"] - hal_last_end > threshold                                 or segment[\"start\"] < threshold                                 or segment[\"start\"] - time_offset < 2.0                             )                             silence_after = (                                 hal_next_start - segment[\"end\"] > threshold                                 or is_segment_anomaly(next_segment)                                 or window_end_time - segment[\"end\"] < 2.0                             )                             if silence_before and silence_after:                                 seek = round(                                     max(time_offset + 1, segment[\"start\"])                                     * self.frames_per_second                                 )                                 if content_duration - segment[\"end\"] < threshold:                                     seek = content_frames                                 current_segments[si:] = []                                 break                         hal_last_end = segment[\"end\"]                  last_word_end = get_end(current_segments)                 if last_word_end is not None:                     last_speech_timestamp = last_word_end              for segment in current_segments:                 tokens = segment[\"tokens\"]                 text = tokenizer.decode(tokens)                  if segment[\"start\"] == segment[\"end\"] or not text.strip():                     continue                  all_tokens.extend(tokens)                 idx += 1                  yield Segment(                     id=idx,                     seek=seek,                     start=segment[\"start\"],                     end=segment[\"end\"],                     text=text,                     tokens=tokens,                     temperature=temperature,                     avg_logprob=avg_logprob,                     compression_ratio=compression_ratio,                     no_speech_prob=result.no_speech_prob,                     words=(                         [Word(**word) for word in segment[\"words\"]]                         if options.word_timestamps                         else None                     ),                 )              if (                 not options.condition_on_previous_text                 or temperature > options.prompt_reset_on_temperature             ):                 if options.condition_on_previous_text:                     self.logger.debug(                         \"Reset prompt. prompt_reset_on_temperature threshold is met %f > %f\",                         temperature,                         options.prompt_reset_on_temperature,                     )                  prompt_reset_since = len(all_tokens)",
      "construct_id": "486fc63d-23dc-4e8d-ba35-51ff51323549",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.548412",
        "updated_at": "2025-06-11T16:24:14.548443",
        "language": "py",
        "file": "faster_whisper/transcribe.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 497,
          "end": 831
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE",
          "RANK 3"
        ]
      }
    },
    {
      "id": "52444860-09b3-4ce2-877c-1ef8a0cfa246",
      "content": "    def generate_with_fallback(         self,         encoder_output: ctranslate2.StorageView,         prompt: List[int],         tokenizer: Tokenizer,         options: TranscriptionOptions,     ) -> Tuple[ctranslate2.models.WhisperGenerationResult, float, float, float]:         decode_result = None         all_results = []         below_cr_threshold_results = []          max_initial_timestamp_index = int(             round(options.max_initial_timestamp / self.time_precision)         )         if options.max_new_tokens is not None:             max_length = len(prompt) + options.max_new_tokens         else:             max_length = self.max_length          if max_length > self.max_length:             raise ValueError(                 f\"The length of the prompt is {len(prompt)}, and the `max_new_tokens` \"                 f\"{max_length - len(prompt)}. Thus, the combined length of the prompt \"                 f\"and `max_new_tokens` is: {max_length}. This exceeds the \"                 f\"`max_length` of the Whisper model: {self.max_length}. \"                 \"You should either reduce the length of your prompt, or \"                 \"reduce the value of `max_new_tokens`, \"                 f\"so that their combined length is less that {self.max_length}.\"             )          for temperature in options.temperatures:             if temperature > 0:                 kwargs = {                     \"beam_size\": 1,                     \"num_hypotheses\": options.best_of,                     \"sampling_topk\": 0,                     \"sampling_temperature\": temperature,                 }             else:                 kwargs = {                     \"beam_size\": options.beam_size,                     \"patience\": options.patience,                 }              result = self.model.generate(                 encoder_output,                 [prompt],                 length_penalty=options.length_penalty,                 repetition_penalty=options.repetition_penalty,                 no_repeat_ngram_size=options.no_repeat_ngram_size,                 max_length=max_length,                 return_scores=True,                 return_no_speech_prob=True,                 suppress_blank=options.suppress_blank,                 suppress_tokens=options.suppress_tokens,                 max_initial_timestamp_index=max_initial_timestamp_index,                 **kwargs,             )[0]              tokens = result.sequences_ids[0]              # Recover the average log prob from the returned score.             seq_len = len(tokens)             cum_logprob = result.scores[0] * (seq_len**options.length_penalty)             avg_logprob = cum_logprob / (seq_len + 1)              text = tokenizer.decode(tokens).strip()             compression_ratio = get_compression_ratio(text)              decode_result = (                 result,                 avg_logprob,                 temperature,                 compression_ratio,             )             all_results.append(decode_result)              needs_fallback = False              if options.compression_ratio_threshold is not None:                 if compression_ratio > options.compression_ratio_threshold:                     needs_fallback = True  # too repetitive                      self.logger.debug(                         \"Compression ratio threshold is not met with temperature %.1f (%f > %f)\",                         temperature,                         compression_ratio,                         options.compression_ratio_threshold,                     )                 else:                     below_cr_threshold_results.append(decode_result)              if (                 options.log_prob_threshold is not None                 and avg_logprob < options.log_prob_threshold             ):                 needs_fallback = True  # average log probability is too low                  self.logger.debug(                     \"Log probability threshold is not met with temperature %.1f (%f < %f)\",                     temperature,                     avg_logprob,                     options.log_prob_threshold,                 )              if (                 options.no_speech_threshold is not None                 and result.no_speech_prob > options.no_speech_threshold                 and options.log_prob_threshold is not None                 and avg_logprob < options.log_prob_threshold             ):                 needs_fallback = False  # silence              if not needs_fallback:                 break         else:             # all failed, select the result with the highest average log probability             decode_result = max(                 below_cr_threshold_results or all_results, key=lambda x: x[1]             )             # to pass final temperature for prompt_reset_on_temperature             decode_result = (                 decode_result[0],                 decode_result[1],                 temperature,                 decode_result[3],             )          return decode_result",
      "construct_id": "1f83819e-4a73-4603-a8cb-b129897845a2",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.549503",
        "updated_at": "2025-06-11T16:24:14.549518",
        "language": "py",
        "file": "faster_whisper/transcribe.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 843,
          "end": 971
        },
        "tokens": null,
        "construct_tags": [
          "RANK 4",
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE"
        ]
      }
    },
    {
      "id": "226b0998-0e4b-4183-b7cd-71ebe92aeec7",
      "content": "    def transcribe(         self,         audio: Union[str, BinaryIO, np.ndarray],         language: Optional[str] = None,         task: str = \"transcribe\",         beam_size: int = 5,         best_of: int = 5,         patience: float = 1,         length_penalty: float = 1,         repetition_penalty: float = 1,         no_repeat_ngram_size: int = 0,         temperature: Union[float, List[float], Tuple[float, ...]] = [             0.0,             0.2,             0.4,             0.6,             0.8,             1.0,         ],         compression_ratio_threshold: Optional[float] = 2.4,         log_prob_threshold: Optional[float] = -1.0,         no_speech_threshold: Optional[float] = 0.6,         condition_on_previous_text: bool = True,         prompt_reset_on_temperature: float = 0.5,         initial_prompt: Optional[Union[str, Iterable[int]]] = None,         prefix: Optional[str] = None,         suppress_blank: bool = True,         suppress_tokens: Optional[List[int]] = [-1],         without_timestamps: bool = False,         max_initial_timestamp: float = 1.0,         word_timestamps: bool = False,         prepend_punctuations: str = \"\\\"'\"\u00bf([{-\",         append_punctuations: str = \"\\\"'.\u3002,,!!??::\")]}\u3001\",         vad_filter: bool = False,         vad_parameters: Optional[Union[dict, VadOptions]] = None,         max_new_tokens: Optional[int] = None,         chunk_length: Optional[int] = None,         clip_timestamps: Union[str, List[float]] = \"0\",         hallucination_silence_threshold: Optional[float] = None,         hotwords: Optional[str] = None,         language_detection_threshold: Optional[float] = None,         language_detection_segments: int = 1,     ) -> Tuple[Iterable[Segment], TranscriptionInfo]:         \"\"\"Transcribes an input file.          Arguments:           audio: Path to the input file (or a file-like object), or the audio waveform.           language: The language spoken in the audio. It should be a language code such             as \"en\" or \"fr\". If not set, the language will be detected in the first 30 seconds             of audio.           task: Task to execute (transcribe or translate).           beam_size: Beam size to use for decoding.           best_of: Number of candidates when sampling with non-zero temperature.           patience: Beam search patience factor.           length_penalty: Exponential length penalty constant.           repetition_penalty: Penalty applied to the score of previously generated tokens             (set > 1 to penalize).           no_repeat_ngram_size: Prevent repetitions of ngrams with this size (set 0 to disable).           temperature: Temperature for sampling. It can be a tuple of temperatures,             which will be successively used upon failures according to either             `compression_ratio_threshold` or `log_prob_threshold`.           compression_ratio_threshold: If the gzip compression ratio is above this value,             treat as failed.           log_prob_threshold: If the average log probability over sampled tokens is             below this value, treat as failed.           no_speech_threshold: If the no_speech probability is higher than this value AND             the average log probability over sampled tokens is below `log_prob_threshold`,             consider the segment as silent.           condition_on_previous_text: If True, the previous output of the model is provided             as a prompt for the next window; disabling may make the text inconsistent across             windows, but the model becomes less prone to getting stuck in a failure loop,             such as repetition looping or timestamps going out of sync.           prompt_reset_on_temperature: Resets prompt if temperature is above this value.             Arg has effect only if condition_on_previous_text is True.           initial_prompt: Optional text string or iterable of token ids to provide as a             prompt for the first window.           prefix: Optional text to provide as a prefix for the first window.           suppress_blank: Suppress blank outputs at the beginning of the sampling.           suppress_tokens: List of token IDs to suppress. -1 will suppress a default set             of symbols as defined in `tokenizer.non_speech_tokens()`           without_timestamps: Only sample text tokens.           max_initial_timestamp: The initial timestamp cannot be later than this.           word_timestamps: Extract word-level timestamps using the cross-attention pattern             and dynamic time warping, and include the timestamps for each word in each segment.           prepend_punctuations: If word_timestamps is True, merge these punctuation symbols             with the next word           append_punctuations: If word_timestamps is True, merge these punctuation symbols             with the previous word           vad_filter: Enable the voice activity detection (VAD) to filter out parts of the audio             without speech. This step is using the Silero VAD model             https://github.com/snakers4/silero-vad.           vad_parameters: Dictionary of Silero VAD parameters or VadOptions class (see available             parameters and default values in the class `VadOptions`).           max_new_tokens: Maximum number of new tokens to generate per-chunk. If not set,             the maximum will be set by the default max_length.           chunk_length: The length of audio segments. If it is not None, it will overwrite the             default chunk_length of the FeatureExtractor.           clip_timestamps:             Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to              process. The last end timestamp defaults to the end of the file.              vad_filter will be ignored if clip_timestamps is used.           hallucination_silence_threshold:             When word_timestamps is True, skip silent periods longer than this threshold              (in seconds) when a possible hallucination is detected           hotwords:             Hotwords/hint phrases to provide the model with. Has no effect if prefix is not None.           language_detection_threshold: If the maximum probability of the language tokens is higher            than this value, the language is detected.           language_detection_segments: Number of segments to consider for the language detection.         Returns:           A tuple with:              - a generator over transcribed segments             - an instance of TranscriptionInfo         \"\"\"         sampling_rate = self.feature_extractor.sampling_rate          if not isinstance(audio, np.ndarray):             audio = decode_audio(audio, sampling_rate=sampling_rate)          duration = audio.shape[0] / sampling_rate         duration_after_vad = duration          self.logger.info(             \"Processing audio with duration %s\", format_timestamp(duration)         )          if vad_filter and clip_timestamps == \"0\":             if vad_parameters is None:                 vad_parameters = VadOptions()             elif isinstance(vad_parameters, dict):                 vad_parameters = VadOptions(**vad_parameters)             speech_chunks = get_speech_timestamps(audio, vad_parameters)             audio = collect_chunks(audio, speech_chunks)             duration_after_vad = audio.shape[0] / sampling_rate              self.logger.info(                 \"VAD filter removed %s of audio\",                 format_timestamp(duration - duration_after_vad),             )              if self.logger.isEnabledFor(logging.DEBUG):                 self.logger.debug(                     \"VAD filter kept the following audio segments: %s\",                     \", \".join(                         \"[%s -> %s]\"                         % (                             format_timestamp(chunk[\"start\"] / sampling_rate),                             format_timestamp(chunk[\"end\"] / sampling_rate),                         )                         for chunk in speech_chunks                     ),                 )          else:             speech_chunks = None          features = self.feature_extractor(audio, chunk_length=chunk_length)          encoder_output = None         all_language_probs = None          if language is None:             if not self.model.is_multilingual:                 language = \"en\"                 language_probability = 1             else:                 if (                     language_detection_segments is None                     or language_detection_segments < 1                 ):                     language_detection_segments = 1                 start_timestamp = (                     float(clip_timestamps.split(\",\")[0])                     if isinstance(clip_timestamps, str)                     else clip_timestamps[0]                 )                 content_frames = (                     features.shape[-1] - self.feature_extractor.nb_max_frames                 )                 seek = (                     int(start_timestamp * self.frames_per_second)                     if start_timestamp * self.frames_per_second < content_frames                     else 0                 )                 end_frames = min(                     seek                     + self.feature_extractor.nb_max_frames                     * language_detection_segments,                     content_frames,                 )                 detected_language_info = {}                 while seek <= end_frames:                     segment = features[                         :, seek : seek + self.feature_extractor.nb_max_frames                     ]                     encoder_output = self.encode(segment)                     # results is a list of tuple[str, float] with language names and                     # probabilities.                     results = self.model.detect_language(encoder_output)[0]                     # Parse language names to strip out markers                     all_language_probs = [                         (token[2:-2], prob) for (token, prob) in results                     ]                     # Get top language token and probability                     language, language_probability = all_language_probs[0]                     if (                         language_detection_threshold is None                         or language_probability > language_detection_threshold                     ):                         break                     detected_language_info.setdefault(language, []).append(                         language_probability                     )                     seek += segment.shape[-1]                 else:                     # If no language detected for all segments, the majority vote of the highest                     # projected languages for all segments is used to determine the language.                     language = max(                         detected_language_info,                         key=lambda lang: len(detected_language_info[lang]),                     )                     language_probability = max(detected_language_info[language])                  self.logger.info(                     \"Detected language '%s' with probability %.2f\",                     language,                     language_probability,                 )         else:             if not self.model.is_multilingual and language != \"en\":                 self.logger.warning(                     \"The current model is English-only but the language parameter is set to '%s'; \"                     \"using 'en' instead.\" % language                 )                 language = \"en\"              language_probability = 1          tokenizer = Tokenizer(             self.hf_tokenizer,             self.model.is_multilingual,             task=task,             language=language,         )          options = TranscriptionOptions(             beam_size=beam_size,             best_of=best_of,             patience=patience,             length_penalty=length_penalty,             repetition_penalty=repetition_penalty,             no_repeat_ngram_size=no_repeat_ngram_size,             log_prob_threshold=log_prob_threshold,             no_speech_threshold=no_speech_threshold,             compression_ratio_threshold=compression_ratio_threshold,             condition_on_previous_text=condition_on_previous_text,             prompt_reset_on_temperature=prompt_reset_on_temperature,             temperatures=(                 temperature if isinstance(temperature, (list, tuple)) else [temperature]             ),             initial_prompt=initial_prompt,             prefix=prefix,             suppress_blank=suppress_blank,             suppress_tokens=(                 get_suppressed_tokens(tokenizer, suppress_tokens)                 if suppress_tokens                 else suppress_tokens             ),             without_timestamps=without_timestamps,             max_initial_timestamp=max_initial_timestamp,             word_timestamps=word_timestamps,             prepend_punctuations=prepend_punctuations,             append_punctuations=append_punctuations,             max_new_tokens=max_new_tokens,             clip_timestamps=clip_timestamps,             hallucination_silence_threshold=hallucination_silence_threshold,             hotwords=hotwords,         )          segments = self.generate_segments(features, tokenizer, options, encoder_output)          if speech_chunks:             segments = restore_speech_timestamps(segments, speech_chunks, sampling_rate)          info = TranscriptionInfo(             language=language,             language_probability=language_probability,             duration=duration,             duration_after_vad=duration_after_vad,             transcription_options=options,             vad_options=vad_parameters,             all_language_probs=all_language_probs,         )         return segments, info",
      "construct_id": "ebb074b8-c8ab-478f-90d1-e4629a9e6f5a",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.573559",
        "updated_at": "2025-06-11T16:24:14.573575",
        "language": "py",
        "file": "faster_whisper/transcribe.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 201,
          "end": 495
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE",
          "RANK 5"
        ]
      }
    },
    {
      "id": "ea32ff93-ad8e-49e6-8747-d680e4547b39",
      "content": "def decode_audio(     input_file: Union[str, BinaryIO],     sampling_rate: int = 16000,     split_stereo: bool = False, ):     \"\"\"Decodes the audio.      Args:       input_file: Path to the input file or a file-like object.       sampling_rate: Resample the audio to this sample rate.       split_stereo: Return separate left and right channels.      Returns:       A float32 Numpy array.        If `split_stereo` is enabled, the function returns a 2-tuple with the       separated left and right channels.     \"\"\"     resampler = av.audio.resampler.AudioResampler(         format=\"s16\",         layout=\"mono\" if not split_stereo else \"stereo\",         rate=sampling_rate,     )      raw_buffer = io.BytesIO()     dtype = None      with av.open(input_file, mode=\"r\", metadata_errors=\"ignore\") as container:         frames = container.decode(audio=0)         frames = _ignore_invalid_frames(frames)         frames = _group_frames(frames, 500000)         frames = _resample_frames(frames, resampler)          for frame in frames:             array = frame.to_ndarray()             dtype = array.dtype             raw_buffer.write(array)      # It appears that some objects related to the resampler are not freed     # unless the garbage collector is manually run.     del resampler     gc.collect()      audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)      # Convert s16 back to f32.     audio = audio.astype(np.float32) / 32768.0      if split_stereo:         left_channel = audio[0::2]         right_channel = audio[1::2]         return left_channel, right_channel      return audio",
      "construct_id": "a5261e68-b7f3-4470-a1b6-9732f7da14df",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.574622",
        "updated_at": "2025-06-11T16:24:14.574636",
        "language": "py",
        "file": "faster_whisper/audio.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 19,
          "end": 72
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE",
          "RANK 6"
        ]
      }
    },
    {
      "id": "1a1a80f5-2f0d-496f-94bb-b2a0135091c6",
      "content": "def _resample_frames(frames, resampler):     # Add None to flush the resampler.     for frame in itertools.chain(frames, [None]):         yield from resampler.resample(frame)",
      "construct_id": "41460976-e924-4109-ac9b-a71129e6aa38",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.575378",
        "updated_at": "2025-06-11T16:24:14.575391",
        "language": "py",
        "file": "faster_whisper/audio.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 101,
          "end": 104
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE",
          "RANK 7"
        ]
      }
    },
    {
      "id": "60406881-5ca9-4a47-a3ad-08c760145e5d",
      "content": "    def __call__(self, waveform, padding=True, chunk_length=None):         \"\"\"         Compute the log-Mel spectrogram of the provided audio, gives similar results         whisper's original torch implementation with 1e-5 tolerance.         \"\"\"         if chunk_length is not None:             self.n_samples = chunk_length * self.sampling_rate             self.nb_max_frames = self.n_samples // self.hop_length          if padding:             waveform = np.pad(waveform, [(0, self.n_samples)])          window = np.hanning(self.n_fft + 1)[:-1]          frames = self.fram_wave(waveform)         stft = self.stft(frames, window=window)         magnitudes = np.abs(stft[:, :-1]) ** 2          filters = self.mel_filters         mel_spec = filters @ magnitudes          log_spec = np.log10(np.clip(mel_spec, a_min=1e-10, a_max=None))         log_spec = np.maximum(log_spec, log_spec.max() - 8.0)         log_spec = (log_spec + 4.0) / 4.0          return log_spec",
      "construct_id": "cc21f755-de9c-4030-a3c7-f9c2cd1e62d7",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.576169",
        "updated_at": "2025-06-11T16:24:14.576182",
        "language": "py",
        "file": "faster_whisper/feature_extractor.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 145,
          "end": 170
        },
        "tokens": null,
        "construct_tags": [
          "RANK 8",
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE"
        ]
      }
    },
    {
      "id": "f6d5c9bb-ee90-424b-b9d4-fdf2ffaba86f",
      "content": "def _group_frames(frames, num_samples=None):     fifo = av.audio.fifo.AudioFifo()      for frame in frames:         frame.pts = None  # Ignore timestamp check.         fifo.write(frame)          if num_samples is not None and fifo.samples >= num_samples:             yield fifo.read()      if fifo.samples > 0:         yield fifo.read()",
      "construct_id": "bcf9e00a-ff69-470d-bea1-8bee3d6d6b9c",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.577059",
        "updated_at": "2025-06-11T16:24:14.577072",
        "language": "py",
        "file": "faster_whisper/audio.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 87,
          "end": 98
        },
        "tokens": null,
        "construct_tags": [
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE",
          "RANK 9"
        ]
      }
    },
    {
      "id": "589d5996-62bc-4f0e-9e2d-bd8a0a74373d",
      "content": "    def stft(self, frames, window):         \"\"\"         Calculates the complex Short-Time Fourier Transform (STFT) of the given framed signal.         Should give the same results as `torch.stft`.         \"\"\"         frame_size = frames.shape[1]         fft_size = self.n_fft          if fft_size is None:             fft_size = frame_size          if fft_size < frame_size:             raise ValueError(\"FFT size must greater or equal the frame size\")         # number of FFT bins to store         num_fft_bins = (fft_size >> 1) + 1          data = np.empty((len(frames), num_fft_bins), dtype=np.complex64)         fft_signal = np.zeros(fft_size)          for f, frame in enumerate(frames):             if window is not None:                 np.multiply(frame, window, out=fft_signal[:frame_size])             else:                 fft_signal[:frame_size] = frame             data[f] = np.fft.fft(fft_signal, axis=0)[:num_fft_bins]         return data.T",
      "construct_id": "162ae28a-ed0b-429f-80a8-7a4a98fe9b50",
      "tags": [],
      "scores": {},
      "metadata": {
        "created_at": "2025-06-11T16:24:14.577897",
        "updated_at": "2025-06-11T16:24:14.577911",
        "language": "py",
        "file": "faster_whisper/feature_extractor.py",
        "name": "original",
        "imports": [],
        "file_operation": "edit",
        "enabled": true,
        "line_numbers": {
          "start": 118,
          "end": 143
        },
        "tokens": null,
        "construct_tags": [
          "RANK 10",
          "RANK 1-10",
          "FUNCTION",
          "PROFILER",
          "SPEEDSCOPE"
        ]
      }
    }
  ]
}